# README for Language Representation Analysis (`lr_analysis.py`)

## 1. Overview and Purpose

`lr_analysis.py` is the main script for a comprehensive analysis pipeline designed to investigate **Linguistic Relativity (LR)**. As detailed in the thesis "Decoding Linguistic Relativity: Bilingual Semantic Embedding Modeling and Cross-Linguistic Cognitive Experiments," LR posits that the language one speaks influences their thought processes and perception of the world. This script operationalizes the study of LR by analyzing linguistic data (typically adjective-noun associations or experimental task outputs generated by `api_calls.py`) to quantify and visualize conceptual differences across various linguistic profiles.

The primary goals of `lr_analysis.py` are to:
1.  Process and structure data generated from Large Language Models (LLMs) that simulate bilingual speakers with diverse native languages (L1) and English (L2) proficiencies.
2.  Transform textual linguistic data into high-dimensional semantic embeddings using various sentence-transformer models.
3.  Perform extensive quantitative analysis on these embeddings, calculating semantic distances (e.g., Cosine Distance, Jaccard Distance) to measure conceptual divergence.
4.  Analyze these distances across different languages, noun categories, L2 proficiency levels, and other linguistic features (like grammatical gender or language family).
5.  Generate a rich set of visualizations (e.g., t-SNE, UMAP, PCA plots, dendrograms, heatmaps) to explore the semantic spaces.
6.  Conduct statistical analyses (e.g., ANOVA, t-tests, effect sizes, correlations) to test hypotheses related to LR and identify significant patterns of cognitive variability.
7.  Organize all outputs systematically for reproducibility and further research.

This script provides the analytical backbone for empirically assessing how linguistic properties might shape conceptualization, using a data-driven, computational approach.

## 2. Core Idea: Computational Analysis of Linguistic Data

This script leverages a Computational Linguistics (CL) approach to analyze data produced by LLMs (via `api_calls.py`). The core idea is to use quantitative methods to explore the nuances of language's influence on conceptual representation.

-   **Input Data**: The script typically processes CSV files generated by `api_calls.py`. This data might include:
    -   Adjectives associated with nouns by LLMs simulating bilingual speakers.
    -   Responses from LLMs to various LR experimental tasks.
-   **Semantic Embeddings as Proxies**: Textual data (e.g., adjectives) is converted into numerical vector embeddings. Aggregated embeddings for nouns or concepts serve as computational proxies for their mental representation, as influenced by the L1 of the simulated speaker.
-   **Quantifiable Distances**:
    -   **Cosine Distance**: Measures the angular difference between embedding vectors, reflecting semantic similarity/dissimilarity.
    -   **Jaccard Distance**: Measures the dissimilarity between sets of adjectives, providing a lexical perspective.
-   **Multi-Level Analysis**: The script facilitates analysis at several granularities:
    -   **Noun-Level**: How the concept of a specific noun (e.g., "freedom," "bridge") varies across languages.
    -   **Category-Level**: Comparing divergence across conceptual categories (e.g., Abstract Concepts vs. Inanimate Objects).
    -   **Language-Level**: Overall semantic distances between aggregated language representations.
    -   **Language Family-Level**: Distances and clustering among language families.
    -   **Proficiency-Level**: How L2 proficiency affects the manifestation of L1 influence.
-   **Statistical Validation**: Rigorous statistical tests are applied to determine if observed differences are significant, and to quantify the magnitude of these differences.

By systematically processing and analyzing this data, `lr_analysis.py` aims to provide empirical insights into the complex interplay between language and thought.

## 3. How `lr_analysis.py` Works: Technical Methodology

The script is organized around a main `analysis_pipeline` function, which coordinates various sub-processes.

### 3.1. Input

-   **User Selections (via `main()` function at runtime):**
    -   **API Provider & Model:** Identifies the LLM provider (e.g., `openai`, `xai`) and specific model (e.g., `gpt-4.1`, `grok-3`) that generated the input data. This is primarily for organizing output directories.
    -   **Input CSV File:** The user navigates to and selects the specific CSV file (output from `api_calls.py`) to be analyzed.
    -   **Embedding Model:** The user selects a sentence-transformer model (e.g., `paraphrase-multilingual-mpnet-base-v2`) from a predefined list in `embedding_models.py`. This model converts text to vector embeddings.
    -   **CSV Delimiter:** Specifies the delimiter used in the input CSV.
-   **Supporting Data Files (loaded from `inputs/` directory):**
    -   `noun_categories.json`: Maps nouns to conceptual categories.
    -   `language_families.json`: Maps language codes to language families.
    -   `grammatical_genders.json`: Maps language codes to grammatical gender systems (e.g., Non-Gendered, Dual-Gendered).

### 3.2. Main Processing Steps (`analysis_pipeline` function)

1.  **Initialization & Directory Setup:**
    -   Creates a unique, structured output directory under `embedding_analysis/` based on the chosen embedding model, API provider, API model, temperature (parsed from the input CSV filename), and the input CSV's base name.
    -   This main directory is further organized into subdirectories: `01_embeddings`, `02_distances`, `03_visualizations`, `04_statistical_reports`.

2.  **Data Loading:**
    -   The selected CSV is loaded into a pandas DataFrame using `load_data` from `lr_analysis_functions.data_loading`. Adjective strings are converted to lists. Noun categories are added. A unique 'Label' is generated for each row.

3.  **Embedding Generation:**
    -   `embedding_models.build_embeddings` is called to convert the relevant textual data (e.g., adjectives) in the DataFrame into numerical embeddings using the chosen sentence-transformer model. The DataFrame is updated with an 'Embedding' column.
    -   The full DataFrame with all embeddings is saved as a pickle file.

4.  **Proficiency-Level Iteration:**
    -   The script iterates through each unique L2 proficiency level (e.g., "A1", "B2", "Perfect") found in the input data.
    -   For each proficiency level:
        -   A proficiency-specific subfolder is created within the main output structure.
        -   The data is filtered for the current proficiency.
        -   The `process_prompt_data` function is called for each unique adjective prompt (e.g., "most frequent," "best") within that proficiency's data.
        -   Proficiency-specific embeddings are saved.

5.  **Prompt-Level and Noun Category-Level Processing (`process_prompt_data` function):**
    -   This function processes data for a specific proficiency and adjective prompt.
    -   It iterates through each unique noun category (e.g., "Abstract Concepts," "Inanimate Objects") and an "all_categories" (combined) case.
    -   **For each Noun Category (and "all_categories"):**
        -   Category-specific sub-subfolders are created under `embeddings`, `distances`, `visualizations`, and `stats_dir`.
        -   Data is filtered for the current noun category.
        -   Category-specific embeddings are saved.
        -   **Distance Calculations:**
            -   `_generate_and_save_noun_level_distances_csv` (from `comprehensive_analysis`): Computes and saves noun-level Cosine and Jaccard distances (comparing different languages for the same noun).
            -   `compute_pairwise_distances` (from `distance_metrics`): Computes a full pairwise Cosine distance matrix for all items in the current category (used for t-SNE/UMAP).
        -   **Visualizations (using functions from `visualization`):**
            -   `generate_standard_dim_reduction_plots`: Creates t-SNE, UMAP, and PCA plots (2D and 3D) for the current category's embeddings. Points are typically colored by 'Language' or 'Noun'.
            -   `plot_language_dendrogram`: Generates dendrograms based on language distances within the category.
            -   `plot_average_noun_differences`: Visualizes average distances for nouns, grouped by category.
            -   `plot_cross_distance_scatter`: Creates scatter plots comparing Cosine vs. Jaccard distances.
            -   `plot_noun_category_language_matrix`: Generates plots showing distances between language pairs for each noun category.
        -   **Language Comparison Data:**
            -   `generate_language_comparisons_df` (from `language_processing`): Creates a DataFrame comparing languages at the noun level within the category.
        -   **Comprehensive Analysis File:**
            -   `generate_combined_analysis` (from `comprehensive_analysis`): Merges noun-level distances, Jaccard distances, and original adjective data into a single comprehensive CSV for the category. These are collected.
        -   **Statistical Analyses (using functions from `statistical_analysis`):**
            -   `analyze_noun_type_distances`: ANOVA and effect sizes for distances across noun categories.
            -   `analyze_gender_language_distances`: ANOVA and effect sizes comparing distances based on the grammatical gender systems of the languages involved.
            -   `calculate_category_percentages` (with `_save_category_percentages_to_csv` from `__init__.py`): Calculates and saves mean distances and uniqueness scores per category.
            -   `analyze_distance_metric_correlation`: Pearson and Spearman correlations between Cosine and Jaccard distances.
            -   `analyze_noun_level_comparisons`: Detailed evaluations of distances for individual nouns across proficiencies, languages, and families.
    -   After iterating through noun categories, combined PCA plots are generated for all noun categories within the current prompt.
    -   Language-level and language-family-level PCA, t-SNE, and UMAP plots are also generated based on the current prompt's data.

6.  **Consolidated Analysis (Post-Proficiency Loop):**
    -   `process_and_save_consolidated_analysis` (from `comprehensive_analysis`): Concatenates all comprehensive analysis DataFrames from each proficiency/prompt/category into a single master CSV.
    -   Further statistical analyses (e.g., gender analysis, correlations) are run on this globally consolidated DataFrame, often including per-language breakdowns.

7.  **Overall Language-Level Analysis:**
    -   `run_language_level_analysis_and_visualization` (from `language_processing`):
        -   Calculates aggregated language embeddings from the full dataset (across all nouns/prompts/proficiencies).
        -   Computes pairwise distances between these overall language representations.
        -   Generates visualizations: dendrograms, heatmaps, PCA, t-SNE, UMAP plots for language embeddings.
        -   Performs statistical analysis on language-level distances.

8.  **Overall Language Family-Level Analysis:**
    -   `run_family_level_analysis_and_visualization` (from `language_processing`):
        -   Calculates aggregated language family embeddings from the language-level embeddings.
        -   Computes pairwise distances between language families.
        -   Generates visualizations for language family embeddings and distances.
        -   Performs statistical analysis on family-level distances.

9.  **Cleanup:**
    -   `embedding_models.clear_model_cache()`: Clears any cached embedding models from memory.

### 3.3. Key Modules and Functions Invoked (from `lr_analysis_functions`)

The `lr_analysis.py` script heavily relies on functions organized into the `lr_analysis_functions` package. The main groups of functions are:

-   **`data_loading`**:
    -   `load_data`: Loads and preprocesses the main input CSV.
    -   `load_noun_categories`, `load_language_families`, `load_grammatical_gender`: Load auxiliary JSON data.
    -   `find_csv_files`, `list_api_providers`, `list_api_models`: Utility functions for file navigation in the `main()` selection process.
-   **`distance_metrics`**:
    -   `compute_pairwise_distances`: Calculates cosine distance matrix between embeddings.
    -   `filter_distances`: Filters distance matrix for specific comparisons (e.g., same noun, different languages).
    -   `compute_jaccard_distances`: Calculates Jaccard distance between sets of adjectives.
    -   `calculate_kruskal_stress`: Measures distortion in dimensionality reduction.
-   **`visualization`**:
    -   `plot_tsne`, `plot_umap`, `plot_pca`: Generate 2D and 3D dimensionality reduction plots (interactive HTML and static PNG).
    -   `plot_language_dendrogram`: Creates hierarchical clustering dendrograms.
    -   `plot_interactive_heatmap`: Generates interactive heatmaps of distance matrices.
    -   `plot_cross_distance_scatter`: Compares different distance metrics via scatter plots.
    -   `plot_noun_category_language_matrix`, `plot_average_noun_differences`, `plot_combined_family_language_pca`: Specialized plots for specific analyses.
    -   `generate_standard_dim_reduction_plots`: A wrapper for creating a common set of t-SNE, UMAP, and PCA plots.
-   **`statistical_analysis`**:
    -   `calculate_cohens_d`: Calculates effect sizes.
    -   `analyze_noun_type_distances`: ANOVA for comparing distances across noun categories.
    -   `calculate_category_percentages`: Computes mean distances and uniqueness scores per category.
    -   `analyze_gender_language_distances`: ANOVA comparing distances based on language gender systems.
    -   `analyze_distance_metric_correlation`: Calculates Pearson/Spearman correlations.
    -   `analyze_noun_level_comparisons`: Detailed noun-specific statistical evaluations.
    -   `analyze_language_level_statistics`, `analyze_language_family_statistics`: ANOVA and descriptive stats for language/family distances.
-   **`language_processing`**:
    -   `calculate_language_embeddings`, `calculate_language_family_embeddings_and_distances`: Aggregate embeddings to language/family levels.
    -   `compute_language_distances`: Calculates distances between aggregated language embeddings.
    -   `generate_language_level_comparisons`, `generate_language_comparisons_df`: Create detailed CSVs comparing language pairs.
    -   `run_language_level_analysis_and_visualization`, `run_family_level_analysis_and_visualization`: Orchestration functions for these levels of analysis.
-   **`comprehensive_analysis`**:
    -   `_generate_and_save_noun_level_distances_csv`: Utility for creating noun-specific distance files.
    -   `generate_combined_analysis`: Merges various data points (distances, adjectives) into a comprehensive CSV for each category/prompt.
    -   `process_and_save_consolidated_analysis`: Aggregates all comprehensive DataFrames into a master file.
-   **`utils`**:
    -   `sanitize_prompt`, `sanitize_category`, `sanitize_name`: Clean strings for file/directory names.
    -   `detect_optimal_device`: (Not directly used by `lr_analysis.py` but part of the ecosystem).
    -   `create_directory_structure`, `create_category_directories`: Create the output folder hierarchy.
-   **`embedding_models.py` (Separate module, not in `lr_analysis_functions`)**:
    -   `load_embedding_models`: Provides a list of available sentence-transformer models.
    -   `build_embeddings`: Loads a chosen model and computes embeddings for input text data.
    -   `clear_model_cache`: Frees GPU/CPU memory by clearing cached models.

### 3.4. Output Structure

The script generates a detailed hierarchy of output files within the `embedding_analysis/<embedding_model_short>/<api_provider>/<api_model>/<temperature>/<input_csv_basename>/` directory. Key subdirectories include:

-   **`01_embeddings/`**: Stores pickled pandas DataFrames containing the input data augmented with their vector embeddings. This includes:
    -   `language_level/`: Aggregated embeddings for each language.
    -   `language_family_level/`: Aggregated embeddings for language families.
    -   `proficiency_specific/`: Embeddings filtered by L2 proficiency level.
    -   `details/<proficiency>/<prompt>/<category>/`: Granular embeddings for specific proficiency-prompt-category combinations.
-   **`02_distances/`**: Contains CSV files of calculated distances:
    -   `comprehensive_analysis/`: Detailed merged data files, including `all_proficiencies_consolidated_...csv`.
    -   `language_level/`: Pairwise distances between overall language representations. Includes `proficiency_specific/` subfolder.
    -   `language_family_level/`: Pairwise distances between language family representations.
    -   `noun_level_distances/details/<proficiency>/<prompt>/<category>/`: Cosine and Jaccard distances for each noun.
    -   `noun_level_language_comparisons/details/<proficiency>/<prompt>/<category>/`: Comparisons between languages at the noun level.
-   **`03_visualizations/`**: A rich collection of plots, mostly in interactive HTML format, with static PNG fallbacks:
    -   `average_noun_differences/`, `dendrograms/`, `heatmaps/`, `noun_category_matrix/`, `pca/`, `scatter_plots/`, `tsne/`, `umap/`.
    -   Many of these have subdirectories for `language_level`, `language_family_level`, and `details/<proficiency>/<prompt>/<category>/` to organize plots by analysis scope and dimensionality reduction technique.
-   **`04_statistical_reports/`**: Text files (`.txt`) and CSVs detailing the results of statistical analyses:
    -   Subdirectories like `category_percentage_analysis/`, `correlations/`, `gender_language_analysis/`, `language_family_statistics/`, `language_level_statistics/`, `noun_level_evaluations/`, `noun_type_analysis/`.
    -   These often contain `details/` and `per_language/` subfolders for granular reports.

## 4. How to Use

The script is designed to be run from the command line.

```bash
python lr_analysis.py
```

Upon execution, the `main()` function initiates an interactive CLI that guides the user through:
1.  **Selecting API Provider:** Lists detected providers from the `api_generations/` directory.
2.  **Selecting API Model:** Lists models available under the chosen provider.
3.  **Selecting Input CSV File:** Allows navigation through the directory structure to pick the target CSV generated by `api_calls.py`.
4.  **Selecting Embedding Model:** Presents a list of sentence-transformer models from `embedding_models.py`.
5.  **Entering CSV Delimiter:** Prompts for the delimiter used in the CSV (defaults to comma).

After these selections, the `analysis_pipeline` function is invoked, and the analysis proceeds, printing progress and status messages to the console. Results are saved to the structured output directory described above. The user can choose to run analyses with different embedding models on the same input data sequentially.

## 5. Dependencies

Key Python libraries required:
-   `pandas`
-   `numpy`
-   `scikit-learn` (for `TSNE`, `PCA`, `cosine_similarity`, `MDS`)
-   `matplotlib` (for static plots and fallbacks)
-   `scipy` (for `linkage` in dendrograms, `stats` for correlations)
-   `statsmodels` (for ANOVA and Tukey's HSD)
-   `sentence-transformers` (used via `embedding_models.py` for embedding generation)
-   `plotly` (for interactive HTML visualizations)
-   `umap-learn` (for UMAP dimensionality reduction)
-   `kaleido` (optional, but recommended for exporting Plotly figures to static PNGs)
-   Standard Python libraries: `os`, `csv`, `json`, `glob`, `time`, `multiprocessing`, `traceback`, `concurrent.futures`, `contextlib`, `re`, `platform`, `torch`, `shutil`, `hashlib`.

It's recommended to use a virtual environment and install these dependencies, for example, using a `requirements.txt` file.

---

This README provides a guide to understanding and using `lr_analysis.py`. For the theoretical framework, detailed experimental design leading to the input data, and interpretation of results, please refer to the main thesis document: "Decoding Linguistic Relativity: Bilingual Semantic Embedding Modeling and Cross-Linguistic Cognitive Experiments." 