# README for Linguistic Relativity Experiments Orchestrator (`lr_experiments.py`)

## 1. Overview and Purpose

`lr_experiments.py` serves as the central orchestrator for running a suite of diverse linguistic relativity (LR) experiments. It integrates and executes various specialized experimental modules found within the `lr_experiment_functions` directory. The core theoretical purpose of this script, and the experiments it manages, is to empirically investigate the Whorfian hypothesis of linguistic relativity: the idea that the structure and lexicon of a language influence an individual's perception, thought processes, and cognitive categorization of the world.

This script facilitates a modular approach to LR research by allowing different cognitive domains (e.g., spatial reasoning, color perception, acoustic reasoning, grammatical gender effects) to be tested independently or collectively. Each experiment, defined in its respective module within `lr_experiment_functions`, is designed to probe specific ways in which language might shape cognition. `lr_experiments.py` provides the framework to run these experiments, typically by leveraging Large Language Models (LLMs) as simulated cognitive agents, as orchestrated by `api_calls.py`.

The primary goals of `lr_experiments.py` are to:
1.  Provide a unified entry point for running various pre-defined LR experiments.
2.  Manage the execution flow for each selected experiment, invoking the appropriate functions from the `lr_experiment_functions` package.
3.  Ensure that experimental data (often generated by `api_calls.py` in response to specific experimental prompts) is loaded and processed according to the logic defined in each experiment's module.
4.  Facilitate the analysis of results from these experiments by calling dedicated analysis functions within each module.
5.  Organize and save the outputs of these analyses in a structured manner, typically within the `lr_experiment_results` directory, creating subdirectories for each experiment type.

By orchestrating these diverse experimental modules, `lr_experiments.py` aims to provide a broad empirical basis for assessing the multifaceted nature of linguistic relativity across different cognitive tasks and linguistic phenomena.

## 2. Core Idea: Modular Experimental Investigation of Linguistic Relativity

The central idea behind `lr_experiments.py` is to enable a structured and reproducible investigation of linguistic relativity through a collection of targeted experiments. Each experiment module in `lr_experiment_functions` focuses on a particular cognitive domain or linguistic feature.

*   **Experimental Modularity**: Different aspects of LR are explored through distinct Python modules:
    *   `acoustic_reasoning_experiment.py`: Investigates how language influences the interpretation of non-linguistic sounds (e.g., associating emotions with sound wave visualizations).
    *   `color_experiment.py`: Examines how color terminology and categorization in different languages affect color perception and association tasks.
    *   `grammatical_gender_experiment.py`: Probes the cognitive impact of grammatical gender systems on object perception, conceptualization, and semantic similarity. This module has two main parts: one analyzing choice preferences in gender-related questions, and another, more extensive part, analyzing semantic distances in embedding spaces.
    *   `preference_tasks.py`: A general module for analyzing experiments where choices or preferences are elicited (e.g., multiple-choice questions, neutral animal associations).
    *   `quantitative_reasoning_experiment.py` (Placeholder): Intended for experiments related to numerical cognition and how language might influence it.
    *   `spatial_reasoning_experiment.py`: Tests how linguistic frameworks for encoding spatial relationships affect spatial problem-solving and route description.
    *   `temporal_reasoning_experiment.py` (Placeholder): Intended for experiments on how language structures the perception and understanding of time.
*   **Data Flow**:
    *   **Input Data**: Experimental results are typically sourced from CSV files located in `api_generations/lr_experiments/[experiment_name]/`. These files are usually generated by `api_calls.py`, where LLMs respond to specific experimental stimuli.
    *   **Experiment Definitions**: Many experiments also rely on JSON definition files (e.g., `inputs/lr_experiments_inputs/[experiment_name].json`) that contain stimuli, correct answers, or task parameters.
*   **Analysis within Modules**: Each experiment module (`*.py` file in `lr_experiment_functions/`) contains functions to:
    *   Load its specific result CSVs.
    *   Load its specific definition/stimuli files.
    *   Parse and score responses according to the task's logic (e.g., accuracy for definitive answers, choice frequencies for preference tasks).
    *   Perform statistical analyses relevant to the experiment (e.g., accuracy metrics, confusion matrices, chi-squared tests, frequency distributions).
    *   Generate visualizations (e.g., bar plots, heatmaps) to illustrate the findings.
*   **Centralized Execution**: `lr_experiments.py` provides a menu-driven interface to select and run the analysis for one or more of these experiments.

This approach allows for focused investigation of specific LR hypotheses while maintaining a consistent overarching framework for experimental execution and result organization.

## 3. How `lr_experiments.py` Works: Technical Methodology

The `lr_experiments.py` script acts as a high-level controller that delegates the actual experimental logic and analysis to the individual modules within the `lr_experiment_functions` folder.

### 3.1. Input

*   **User Selections (via `main()` function at runtime):**
    *   The user is presented with a menu listing the available LR experiments (derived from the callable `run_*_analysis` or `analyze_*` functions within the imported modules).
    *   The user can choose to run a specific experiment, all experiments, or exit.
*   **Data Files (handled by individual experiment modules):**
    *   **Experiment Results CSVs**: Located in `api_generations/lr_experiments/[experiment_name]/`. Each module knows how to find and load its relevant CSVs (e.g., `acoustic_reasoning_experiment.py` looks in `api_generations/lr_experiments/sound_waves/`).
    *   **Experiment Definition/Stimuli JSONs**: Located in `inputs/lr_experiments_inputs/`. Each module loads its specific JSON (e.g., `spatial_reasoning_experiment.json`).

### 3.2. Main Processing Steps (`main()` function in `lr_experiments.py`)

1.  **Module Discovery and Function Mapping:**
    *   The script imports `run_*_analysis` or `analyze_*` functions from each module in the `lr_experiment_functions` directory (e.g., `run_acoustic_reasoning_analysis` from `acoustic_reasoning_experiment.py`).
    *   It creates a mapping between user-friendly experiment names and these callable functions.

2.  **User Interaction (Menu):**
    *   Displays a numbered list of available experiments.
    *   Prompts the user to select an experiment to analyze or an option to run all.

3.  **Experiment Execution:**
    *   Based on user selection, the corresponding `run_*_analysis()` or `analyze_*()` function from the relevant module is called.
    *   **Each individual experiment module then typically performs the following:**
        *   **Load Data**: Reads its specific CSV result files (e.g., using `load_acoustic_results()` in `acoustic_reasoning_experiment.py`).
        *   **Load Definitions (if applicable)**: Reads its JSON experiment definition file (e.g., `load_spatial_experiment_definition()` in `spatial_reasoning_experiment.py`).
        *   **Parse Responses**: Cleans and structures the raw responses from the LLM (e.g., `parse_and_score_color_responses()` in `color_experiment.py`).
        *   **Score Responses (if applicable)**: For experiments with correct/optimal answers, it scores the parsed responses against ground truth (e.g., `score_acoustic_responses()` in `acoustic_reasoning_experiment.py`).
        *   **Perform Analysis**: Conducts statistical analysis appropriate for the experiment type. This might include:
            *   Calculating accuracy, precision, recall, F1-scores, confusion matrices (e.g., `analyze_acoustic_accuracy` in `acoustic_reasoning_experiment.py`).
            *   Analyzing choice frequencies and distributions (e.g., `analyze_choice_frequencies` in `preference_tasks.py`).
            *   Comparing performance across different languages, models, or system prompt types.
        *   **Generate Visualizations**: Creates plots like bar charts for accuracy, heatmaps for confusion matrices, or count plots for choice frequencies. These are typically saved to `lr_experiment_results/[experiment_name]_analysis/`.
        *   **Save Outputs**: Saves summary statistics, reports, and figures to dedicated directories.

4.  **Loop or Exit:**
    *   After an experiment's analysis is complete, the script can either return to the menu (if not running all) or exit.

### 3.3. Key Modules and Functions Invoked (from `lr_experiment_functions/`)

`lr_experiments.py` itself is lean; the bulk of the work is done by functions within these modules:

*   **`acoustic_reasoning_experiment.py`**:
    *   `load_acoustic_results()`: Loads sound wave experiment CSVs.
    *   `score_acoustic_responses()`: Scores emotion predictions against ground truth.
    *   `analyze_acoustic_accuracy()`: Calculates and plots accuracy, generates confusion matrices and classification reports.
    *   `run_acoustic_reasoning_analysis()`: Orchestrates the analysis for this experiment.
*   **`color_experiment.py`**:
    *   `load_color_experiment_results()`: Loads color experiment CSVs.
    *   `load_color_experiment_definition()`: Loads color experiment JSON.
    *   `parse_and_score_color_responses()`: Parses and scores responses (accuracy-based).
    *   `analyze_color_accuracy()`: Analyzes and plots accuracy for tasks with correct answers.
    *   `analyze_color_choice_frequencies()`: Analyzes and plots frequencies for association/scenario tasks.
    *   `run_color_experiment_analysis()`: Main analysis function for color experiments.
*   **`grammatical_gender_experiment.py`**:
    *   **New Preference Task Analysis Part**:
        *   `load_gg_question_results()`: Loads CSVs for grammatical gender preference questions.
        *   `parse_gg_question_responses()`: Parses LLM choices.
        *   `analyze_gg_question_choice_frequencies()`: Analyzes choice frequencies for these questions.
        *   `run_grammatical_gender_questions_analysis()`: Orchestrates the preference task analysis.
    *   **Existing Embedding Analysis Part (Extensive)**:
        *   Numerous functions for loading gender maps (`load_grammatical_genders`), distance data (`load_distance_data`), processing pairs (`process_gender_category_pair`, `process_language_pair`), statistical tests (`calculate_cohens_d`, `permutation_test_gender_systems`), visualizations (`plot_gender_system_heatmap`, `gender_system_clustering`), and running the overall embedding-based analysis (`run_grammatical_gender_distance_analysis`, `analyze_new_embedding_data`).
*   **`preference_tasks.py`**:
    *   `load_experiment_results()`: Generic function to load CSVs for any preference task experiment.
    *   `parse_responses_from_experiment()`: Parses responses.
    *   `analyze_choice_frequencies()`: Generic function to analyze and plot choice frequencies.
    *   Specific `analyze_*_experiment()` functions (e.g., `analyze_multiple_choice_experiment()`, `analyze_neutral_animals_experiment()`) that use the generic helpers.
*   **`quantitative_reasoning_experiment.py`** (Placeholder):
    *   `load_quantitative_results()`: Placeholder for loading data.
    *   `analyze_quantitative_reasoning()`: Placeholder for analysis logic.
*   **`spatial_reasoning_experiment.py`**:
    *   `load_spatial_experiment_results()`: Loads spatial experiment CSVs.
    *   `load_spatial_experiment_definition()`: Loads spatial experiment JSON.
    *   `parse_and_score_spatial_responses()`: Parses responses and scores optimality/path length.
    *   `analyze_spatial_accuracy_optimality()`: Analyzes and plots optimality rates.
    *   `run_spatial_reasoning_analysis()`: Orchestrates the analysis.
*   **`temporal_reasoning_experiment.py`** (Placeholder):
    *   `load_temporal_results()`: Placeholder for loading data.
    *   `analyze_temporal_reasoning()`: Placeholder for analysis logic.
*   **`lr_experiments_results_analyzer.py` (Advanced Overall Analyzer)**:
    *   While `lr_experiments.py` focuses on running individual experiment modules, `LRExperimentAnalyzer` class in `lr_experiments_results_analyzer.py` provides a more comprehensive, cross-experiment, and academically rigorous analysis layer *on top of* the results generated by these individual modules. It can perform meta-analyses, deeper statistical tests (mixed-effects models, Bayesian analysis, power analysis), generate publication-ready figures, and provide detailed academic reports on linguistic relativity. It's designed to be run *after* `lr_experiments.py` (or the individual modules) have produced their initial set of results.
*   **`simple_lr_experiment_results.py` (Simplified Analyzer)**:
    *   This script offers a more focused and streamlined approach to analyzing experiment results, particularly emphasizing cross-linguistic variance and pairwise comparisons.
    *   It includes interactive selection for experiments and specific LLM generations.
    *   Key analyses include calculating cross-linguistic variance in responses, performing pairwise Chi-square tests, calculating Cohen's d for effect sizes, and generating visualizations like heatmaps and variance bar charts.
    *   It produces summary reports and detailed CSVs for response percentages and pairwise comparisons.
    *   This analyzer can be useful for quicker, targeted insights into specific datasets or for analyses where the full academic rigor of `lr_experiments_results_analyzer.py` might not be immediately required.
*   **`ch4_report_generator.py` (Automated Thesis-Style Report Generator - formerly `ch4.py`)**:
    *   This script serves as a final consolidation step, designed to automatically generate a structured, thesis-style report (e.g., for a "Chapter 4" results section).
    *   It discovers and aggregates outputs from various experiments, including academic summaries, figures, statistical data (like Cohen's d values and LR assessments) previously generated by `LRExperimentAnalyzer` and individual experiment modules.
    *   It uses a configurable mapping (`SECTION_MAP`) to organize experiment results into predefined thematic sections and subsections.
    *   The script constructs a detailed Markdown document, embedding relative paths to figures, summarizing key statistical findings, and incorporating narrative text from academic summaries.
    *   It aims to streamline the creation of a comprehensive results chapter by programmatically assembling and formatting the diverse outputs from the entire analysis pipeline.

### 3.4. Output Structure

Each experiment module typically saves its outputs to a subdirectory within `lr_experiment_results/`. For example:
*   `lr_experiment_results/acoustic_reasoning_analysis/`
*   `lr_experiment_results/color_reasoning_analysis/`
*   `lr_experiment_results/grammatical_gender_questions_analysis/` (for preference tasks)
*   `lr_experiment_results/grammatical_gender/` (for embedding-based gender analysis, often with subfolders per embedding configuration)
*   `lr_experiment_results/spatial_reasoning_analysis/`

Outputs usually include:
*   **PNG images**: Visualizations like bar plots, heatmaps.
*   **Text files (`.txt`)**: Classification reports, statistical summaries.
*   **CSVs**: Sometimes aggregated or processed data tables.

The more advanced `LRExperimentAnalyzer` from `lr_experiments_results_analyzer.py` has its own extensive output structure under `lr_experiment_results/` (e.g., `academic_reports/`, `publication_ready_figures/`, `statistical_significance/`).

## 4. How to Use

The script is designed to be run from the command line:

```bash
python lr_experiments.py
```

Upon execution:
1.  A menu is displayed, listing the available Linguistic Relativity experiments that can be analyzed. These are dynamically sourced from the imported functions.
2.  The user is prompted to enter the number corresponding to the experiment they wish to analyze, or an option to run all available experiments.
3.  If a specific experiment is chosen, its dedicated `run_*_analysis()` or `analyze_*()` function is executed. This function will then handle loading the necessary data, performing the analysis, and saving the results for that particular experiment.
4.  If "Run all" is selected, the script will iterate through all available experiments and execute their analysis functions sequentially.
5.  Progress and key findings for each experiment are typically printed to the console by the respective modules.

This script acts as a convenient way to trigger and manage the analyses for the various LR experiments developed. For more in-depth, cross-experiment synthesis and advanced statistical reporting, the `LRExperimentAnalyzer` in `lr_experiments_results_analyzer.py` should be used subsequently.

## 5. Dependencies

Key Python libraries required by the various experiment modules include:
*   `pandas`
*   `numpy`
*   `scikit-learn` (for metrics like `accuracy_score`, `confusion_matrix`, `classification_report`)
*   `matplotlib`
*   `seaborn`
*   `scipy` (for statistical tests like `chi2_contingency`, `mannwhitneyu`)
*   Standard Python libraries: `os`, `glob`, `json`.

It's recommended to use a virtual environment and install these dependencies, for example, using a `requirements.txt` or `requirements_enhanced.txt` file from the project.

---

This README provides a guide to understanding and using `lr_experiments.py` as an orchestrator for various linguistic relativity experiments. Each experiment module within `lr_experiment_functions/` contains the specific logic for its domain. For theoretical background and detailed interpretation of results, please refer to the main thesis document: "Decoding Linguistic Relativity: Bilingual Semantic Embedding Modeling and Cross-Linguistic Cognitive Experiments." 