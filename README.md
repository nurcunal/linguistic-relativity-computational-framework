## Linguistic Relativity Computational Framework

### Overview
This repository accompanies the master's thesis **"A Computational Approach to Linguistic Relativity with Empirical Experimentation."**
Its central research question is whether – and to what measurable degree – the
native language of a (simulated) speaker influences conceptual thought.  
To test this, the project combines **large-scale language–model prompting**,
**semantic field construction** and **controlled cognitive experiments** into one
reproducible Python framework.


| Building block | Purpose | Main entry point |
|----------------|---------|------------------|
| **1. api_calls** | Prompt commercial LLMs as *bilingual proxies* to generate adjective lists for 144 nouns across 24 languages × 7 proficiency levels.  Output ⇒ CSV files in `api_generations/…` | `api_calls.py` |
| **2. lr_analysis** | Embed the generated adjectives, compute cross-linguistic distance matrices, visualise the high-dimensional semantic space and run statistical tests.  Output ⇒ embeddings, heat-maps, PCA/UMAP/t-SNE plots, and ANOVA / Cohen-d reports | `lr_analysis.py` |
| **3. lr_experiments** | Run scalable cognitive tasks (multiple-choice, spatial, colour, sound-wave) with the same bilingual LLM configuration; checkpoint & save responses for later analysis. Output ⇒ task-specific CSVs in `api_generations/lr_experiments_english/…` | `lr_experiments.py` |

The three layers share the **same prompt-engineering logic** and a common
folder layout so they can be executed independently *or* as an end-to-end
pipeline:

```
┌── api_calls ──► adjective CSVs
│                (semantic raw data)
│
├── lr_analysis ──► semantic maps & stats
│   ▲
│   │   (feeds noun-, language- & family-level insights)
│
└── lr_experiments ──► behavioural task outputs ▬▬▬▬▬▬┐
        ▲                                              │
        └────────────────── synthesised in thesis ←─────┘
```

---

### Theoretical Rationale
Traditional Linguistic Relativity (LR) studies suffer from **small samples and
subjective interpretation**.  By treating powerful LLMs as corpora of
probabilistic linguistic behaviour we can:

1. **Scale** to millions of adjective–noun associations (≈7.8 M in this repo).
2. **Control** bilingual context via explicit system prompts (Basic-Bilingual vs
   Think-in-Native).
3. **Quantify** conceptual distance with cosine/Jaccard metrics in an embedding
   space instead of anecdotal assessment.
4. **Replicate & extend** classic LR experiments (colour, spatial, grammatical
   gender) without human-participant bottlenecks.

If semantic distances (module 2) and experimental outcomes (module 3) correlate,
this supports the weak form of LR: language guides—but does not strictly
determine—thought.

---

### Quick Start
1. **Clone** the repository and create a virtual environment
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
2. **Add API keys** (one per line) to:
```
inputs/api_key_openai.txt
inputs/api_key_xai.txt
inputs/api_key_google.txt
```
3. **Generate adjective data**
```bash
python api_calls.py
```
Follow the CLI prompts to pick a provider/model/temperature/system-prompt type.
Checkpointed CSVs will accumulate under `api_generations/…`.

4. **Analyse semantic distances**
```bash
python lr_analysis.py
```
Choose the same provider/model and point the interactive file-picker to one of
the CSVs generated in step 3.  Visualisations & stats are written to
`embedding_analysis/…`.

5. **Run cognitive experiments** (optional/orthogonal)
```bash
python lr_experiments.py
```
Select any of the six experiment categories – results are stored with automatic
checkpointing so long runs can be resumed safely.

---

### Folder Structure (partial)
```
├── api_calls.py               # adjective generation pipeline
├── lr_analysis.py             # semantic analysis & visualisation
├── lr_experiments.py          # cognitive task launcher
├── inputs/                    # prompt templates, noun lists, API keys …
├── api_generations/           # *all* raw LLM outputs live here
│   └── lr_experiments_english/
├── embedding_analysis/        # embeddings, distance CSVs, plots
├── readme/                    # this file and supplementary docs
└── requirements.txt
```

---

### Reproducibility
* Every long-running script writes **checkpoints** (per language) so you can
  interrupt with `CTRL-C` and resume later.
* All numeric tables include random-seed metadata when randomness is involved
  (only relevant for t-SNE/UMAP).
* Figures listed in Appendix E of the thesis can be regenerated by re-running
  the same configuration (see names encoded in output filenames).

---

### Citing / Acknowledging
If you use this framework or its results, please cite the thesis:
```
@mastersthesis{Unal2025ComputationalLR,
  title  = {A Computational Approach to Linguistic Relativity with Empirical Experimentation},
  author = {Nureddin Cüneyd Ünal},
  school = {Boğaziçi University},
  year   = {2025}
}
```

---

### Contact & Support
For questions open an issue or contact the author
<nurcunal@proton.me>.  Pull-requests improving code quality or adding new
experiments are welcome! 
